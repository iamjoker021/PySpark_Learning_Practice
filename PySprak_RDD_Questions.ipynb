{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8a777c",
   "metadata": {},
   "source": [
    "Sure, here are 25 beginner to intermediate level assignment questions on PySpark RDDs:\n",
    "- Create an RDD from a list of integers and print the RDD.\n",
    "- Create an RDD from a text file and print the first 10 lines.\n",
    "- Count the number of elements in an RDD.\n",
    "- Find the sum of all elements in an RDD of integers.\n",
    "- Filter out the even numbers from an RDD of integers.\n",
    "- Map the values of an RDD of integers to their squares.\n",
    "- Reduce an RDD of integers to their product.\n",
    "- Calculate the average of elements in an RDD of integers.\n",
    "- Find the maximum value in an RDD of integers.\n",
    "- Find the minimum value in an RDD of integers.\n",
    "- Calculate the standard deviation of elements in an RDD of integers.\n",
    "- Create a new RDD by joining two existing RDDs.\n",
    "- Find the common elements between two RDDs.\n",
    "- Subtract the elements of one RDD from another RDD.\n",
    "- Compute the Cartesian product of two RDDs.\n",
    "- Group the elements of an RDD by key.\n",
    "- Sort an RDD of tuples by key.\n",
    "- Create an RDD of key-value pairs from a text file and count the number of occurrences of each key.\n",
    "- Find the most frequently occurring value in an RDD.\n",
    "- Apply a function to each element of an RDD using mapPartitions.\n",
    "- Combine the values of an RDD using reduceByKey.\n",
    "- Compute the average of values for each key in an RDD of key-value pairs using combineByKey.\n",
    "- Find the top 5 values for each key in an RDD of key-value pairs.\n",
    "- Compute the frequency distribution of elements in an RDD.\n",
    "- Find the unique elements in an RDD.\n",
    "\n",
    "These questions should give you a good foundation in using PySpark RDDs. As you progress, you can try more advanced tasks such as graph processing, machine learning, and streaming data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f03626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd215d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/04 15:35:32 WARN Utils: Your hostname, joker021-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/05/04 15:35:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/04 15:35:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"practice_rdd\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a66af40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1], [2, 3], [4, 5], [6, 7, 8, 9]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD from a list of integers and print the RDD.\n",
    "l = list(range(10))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1392467b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is ',\n",
       " 'random text file',\n",
       " 'where I use to ',\n",
       " 'parse the',\n",
       " 'data',\n",
       " 'in ',\n",
       " 'PySprak and ',\n",
       " 'I forgot what is was thinking',\n",
       " 'Ahh yesss',\n",
       " 'I need to show first 10 lines in ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD from a text file and print the first 10 lines.\n",
    "no_of_lines = 10\n",
    "path = \"spark_text_file.txt\"\n",
    "rdd = spark.sparkContext.textFile(path)\n",
    "rdd.take(no_of_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f093cdb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/home/joker021/Documents/Practice_Spark/Spark_ChatGPT/spark_text_file (another copy).txt',\n",
       "  'this is \\nrandom text file\\nwhere I use to \\nparse the\\ndata\\nin \\nPySprak and \\nI forgot what is was thinking\\nAhh yesss\\nI need to show first 10 lines in \\nRdd as outpur\\nDid we wtote 10 lines?\\ni dont know lets see\\n'),\n",
       " ('file:/home/joker021/Documents/Practice_Spark/Spark_ChatGPT/spark_text_file (copy).txt',\n",
       "  'this is \\nrandom text file\\nwhere I use to \\nparse the\\ndata\\nin \\nPySprak and \\nI forgot what is was thinking\\nAhh yesss\\nI need to show first 10 lines in \\nRdd as outpur\\nDid we wtote 10 lines?\\ni dont know lets see\\n'),\n",
       " ('file:/home/joker021/Documents/Practice_Spark/Spark_ChatGPT/spark_text_file.txt',\n",
       "  'this is \\nrandom text file\\nwhere I use to \\nparse the\\ndata\\nin \\nPySprak and \\nI forgot what is was thinking\\nAhh yesss\\nI need to show first 10 lines in \\nRdd as outpur\\nDid we wtote 10 lines?\\ni dont know lets see\\n')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD from a text file and print the first 10 lines.\n",
    "no_of_lines = 10\n",
    "path = \"*.txt\"\n",
    "rdd = spark.sparkContext.wholeTextFiles(path)\n",
    "rdd.take(no_of_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f1e92d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of elements in an RDD.\n",
    "l = list(range(10))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88c9475b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the sum of all elements in an RDD of integers.\n",
    "l = list(range(10))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "550e2463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [2], [4], [6, 8]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out the even numbers from an RDD of integers.\n",
    "l = list(range(10))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd_even_num = rdd.filter(lambda x: x%2==0)\n",
    "rdd_even_num.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "487531bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1], [4, 9], [16, 25], [36, 49, 64, 81]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the values of an RDD of integers to their squares.\n",
    "l = list(range(10))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd_sqr_map = rdd.map(lambda x: x**2)\n",
    "rdd_sqr_map.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5678ee75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3628800"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce an RDD of integers to their product.\n",
    "l = list(range(1, 10+1))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd_prod = rdd.reduce(lambda a, b: a*b)\n",
    "rdd_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35fab883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average of elements in an RDD of integers.\n",
    "l = list(range(10))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cdb0dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the maximum value in an RDD of integers.\n",
    "l = list(range(10))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd.max()\n",
    "\n",
    "# Using Reduce\n",
    "# rdd.reduce(lambda a,b: a if a>b else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8442bdfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the minimum value in an RDD of integers.\n",
    "l = list(range(10))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a21d19ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8722813232690143"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the standard deviation of elements in an RDD of integers.\n",
    "l = list(range(10))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b883bbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 100, 99, 98, 97, 96, 95, 94, 93, 92, 91]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new RDD by joining two existing RDDs.\n",
    "l1 = list(range(10))\n",
    "rdd1 = spark.sparkContext.parallelize(l1)\n",
    "# rdd1_key = rdd1.map(lambda x: (1, x))\n",
    "\n",
    "l2 = list(range(100, 90, -1))\n",
    "rdd2 = spark.sparkContext.parallelize(l2)\n",
    "# rdd2_key = rdd2.map(lambda x: (1, x))\n",
    "\n",
    "rdd_join = rdd1.union(rdd2)\n",
    "\n",
    "rdd_join.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d112b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (0, 100)),\n",
       " (8, (8, 92)),\n",
       " (1, (1, 99)),\n",
       " (9, (9, 91)),\n",
       " (2, (2, 98)),\n",
       " (3, (3, 97)),\n",
       " (4, (4, 96)),\n",
       " (5, (5, 95)),\n",
       " (6, (6, 94)),\n",
       " (7, (7, 93))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new RDD by joining two existing RDDs.\n",
    "l1 = list(range(10))\n",
    "rdd1 = spark.sparkContext.parallelize(l1)\n",
    "# rdd1_key = rdd1.map(lambda x: (1, x))\n",
    "rdd1_key = rdd1.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "\n",
    "l2 = list(range(100, 90, -1))\n",
    "rdd2 = spark.sparkContext.parallelize(l2)\n",
    "# rdd2_key = rdd2.map(lambda x: (1, x))\n",
    "rdd2_key = rdd2.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "\n",
    "rdd_join = rdd1_key.join(rdd2_key)\n",
    "\n",
    "rdd_join.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "995d0a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (3, 3),\n",
       " (4, 4),\n",
       " (5, 5),\n",
       " (6, 6),\n",
       " (7, 7),\n",
       " (8, 8),\n",
       " (9, 9)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.zipWithIndex().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "744b73da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8], [9], [], [], [4], [5], [6], [7]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the common elements between two RDDs.\n",
    "l1 = list(range(10))\n",
    "rdd1 = spark.sparkContext.parallelize(l1)\n",
    "\n",
    "l2 = list(range(10, 3, -1))\n",
    "rdd2 = spark.sparkContext.parallelize(l2)\n",
    "\n",
    "rdd_common = rdd1.intersection(rdd2)\n",
    "rdd_common.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5ab4c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [1], [2], [3], [], [], [], []]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtract the elements of one RDD from another RDD.\n",
    "# Find the common elements between two RDDs.\n",
    "l1 = list(range(10))\n",
    "rdd1 = spark.sparkContext.parallelize(l1)\n",
    "\n",
    "l2 = list(range(10, 3, -1))\n",
    "rdd2 = spark.sparkContext.parallelize(l2)\n",
    "\n",
    "rdd1_only = rdd1.subtract(rdd2)\n",
    "rdd1_only.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b555c00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 10), (1, 10)],\n",
       " [(0, 9), (1, 9), (0, 8), (1, 8)],\n",
       " [(0, 7), (1, 7), (0, 6), (1, 6)],\n",
       " [(0, 5), (1, 5), (0, 4), (1, 4)],\n",
       " [(2, 10), (3, 10)],\n",
       " [(2, 9), (3, 9), (2, 8), (3, 8)],\n",
       " [(2, 7), (3, 7), (2, 6), (3, 6)],\n",
       " [(2, 5), (3, 5), (2, 4), (3, 4)],\n",
       " [(4, 10), (5, 10)],\n",
       " [(4, 9), (5, 9), (4, 8), (5, 8)],\n",
       " [(4, 7), (5, 7), (4, 6), (5, 6)],\n",
       " [(4, 5), (5, 5), (4, 4), (5, 4)],\n",
       " [(6, 10), (7, 10), (8, 10), (9, 10)],\n",
       " [(6, 9), (7, 9), (6, 8), (7, 8), (8, 9), (9, 9), (8, 8), (9, 8)],\n",
       " [(6, 7), (7, 7), (6, 6), (7, 6), (8, 7), (9, 7), (8, 6), (9, 6)],\n",
       " [(6, 5), (7, 5), (6, 4), (7, 4), (8, 5), (9, 5), (8, 4), (9, 4)]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the Cartesian product of two RDDs.\n",
    "l1 = list(range(10))\n",
    "rdd1 = spark.sparkContext.parallelize(l1)\n",
    "\n",
    "l2 = list(range(10, 3, -1))\n",
    "rdd2 = spark.sparkContext.parallelize(l2)\n",
    "\n",
    "rdd_cartesian = rdd1.cartesian(rdd2)\n",
    "\n",
    "rdd_cartesian.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3fa803a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, [0, 0]),\n",
       " (8, [8, 8]),\n",
       " (1, [1, 1]),\n",
       " (9, [9, 9]),\n",
       " (2, [2, 2]),\n",
       " (3, [3, 3]),\n",
       " (4, [4, 4]),\n",
       " (5, [5, 5]),\n",
       " (6, [6, 6]),\n",
       " (7, [7, 7])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the elements of an RDD by key.\n",
    "l1 = list(range(10))\n",
    "rdd1 = spark.sparkContext.parallelize(l1)\n",
    "# rdd1_key = rdd1.map(lambda x: (1, x))\n",
    "rdd1_key = rdd1.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "\n",
    "l2 = list(range(10))\n",
    "rdd2 = spark.sparkContext.parallelize(l2)\n",
    "# rdd2_key = rdd2.map(lambda x: (1, x))\n",
    "rdd2_key = rdd2.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "\n",
    "rdd_all = rdd1_key.union(rdd2_key)\n",
    "\n",
    "rdd_all.groupByKey().map(lambda x: (x[0], list(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9d65abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 10), (1, 9), (2, 8)],\n",
       " [(3, 7), (4, 6), (5, 5)],\n",
       " [(6, 4), (7, 3)],\n",
       " [(8, 2), (9, 1)]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort an RDD of tuples by key\n",
    "l = list(range(10, 0, -1))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd_k = rdd.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "rdd_k.sortByKey().glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8c32a3a4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'this': 1,\n",
       "             'is': 2,\n",
       "             'random': 1,\n",
       "             'text': 1,\n",
       "             'file': 1,\n",
       "             'where': 1,\n",
       "             'I': 3,\n",
       "             'use': 1,\n",
       "             'to': 2,\n",
       "             'parse': 1,\n",
       "             'the': 1,\n",
       "             'data': 1,\n",
       "             'in': 2,\n",
       "             'PySprak': 1,\n",
       "             'and': 1,\n",
       "             'forgot': 1,\n",
       "             'what': 1,\n",
       "             'was': 1,\n",
       "             'thinking': 1,\n",
       "             'Ahh': 1,\n",
       "             'yesss': 1,\n",
       "             'need': 1,\n",
       "             'show': 1,\n",
       "             'first': 1,\n",
       "             '10': 2,\n",
       "             'lines': 1,\n",
       "             'Rdd': 1,\n",
       "             'as': 1,\n",
       "             'outpur': 1,\n",
       "             'Did': 1,\n",
       "             'we': 1,\n",
       "             'wtote': 1,\n",
       "             'lines?': 1,\n",
       "             'i': 1,\n",
       "             'dont': 1,\n",
       "             'know': 1,\n",
       "             'lets': 1,\n",
       "             'see': 1})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD of key-value pairs from a text file and count the number of occurrences of each key.\n",
    "path = \"spark_text_file.txt\"\n",
    "rdd = spark.sparkContext.textFile(path)\n",
    "rdd_word = rdd.flatMap(lambda x: x.split())\n",
    "# rdd_word.countByValue()\n",
    "rdd_word.map(lambda x: (x, 1)).countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e17cc87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Iam', 6)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most frequently occurring value in an RDD.\n",
    "# Create an RDD of key-value pairs from a text file and count the number of occurrences of each key.\n",
    "path = \"spark_text_file.txt\"\n",
    "rdd = spark.sparkContext.textFile(path)\n",
    "rdd_word = rdd.flatMap(lambda x: x.split())\n",
    "# rdd_word.countByValue()\n",
    "rdd_word_map = rdd_word.map(lambda x: (x, 1))\n",
    "# rdd_word.map(lambda x: (x, 1)).countByKey()\n",
    "rdd_word_count = rdd_word_map.reduceByKey(lambda a,b:a+b)\n",
    "\n",
    "rdd_word_count.max(lambda x: x[1])\n",
    "# rdd_word_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "10c51b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [5], [9], [30]]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a function to each element of an RDD using mapPartitions.\n",
    "l = list(range(10))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "# rdd.mapPartitions(lambda x: sum(x)).glom().collect()\n",
    "# rdd.mapPartitions(lambda it: (x*x for x in it)).glom().collect()\n",
    "rdd.mapPartitions(lambda it: [sum(it)]).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6aa1f006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 20)], [(1, 25)], [], []]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the values of an RDD using reduceByKey.\n",
    "l = list(range(10))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd_odd_even = rdd.map(lambda x: (x%2, x))\n",
    "rdd_odd_even.reduceByKey(lambda a,b:a+b).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "070a881c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4.0), (1, 5.0)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the average of values for each key in an RDD of key-value pairs using combineByKey.\n",
    "l = list(range(10))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "\n",
    "# rdd_odd_even.combineByKey(createCombiner=lambda x: x, mergeValue=lambda a,b:a+b, mergeCombiners=lambda a,b:a+b).collect()\n",
    "rdd_odd_even.combineByKey(\n",
    "    createCombiner=lambda x: (x, 1), \n",
    "    mergeValue=lambda a, b: (a[0]+b, a[1]+1), \n",
    "    mergeCombiners=lambda a, b: (a[0]+b[0], a[1]+b[1])\n",
    ").map(lambda x: (x[0], x[1][0]/x[1][1])).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e6b65072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [90, 80, 70, 60, 50]),\n",
       " (4, [94, 84, 74, 64, 54]),\n",
       " (8, [98, 88, 78, 68, 58]),\n",
       " (1, [91, 81, 71, 61, 51]),\n",
       " (5, [95, 85, 75, 65, 55]),\n",
       " (9, [99, 89, 79, 69, 59]),\n",
       " (2, [92, 82, 72, 62, 52]),\n",
       " (6, [96, 86, 76, 66, 56]),\n",
       " (3, [93, 83, 73, 63, 53]),\n",
       " (7, [97, 87, 77, 67, 57])]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the top 5 values for each key in an RDD of key-value pairs.\n",
    "l = list(range(100))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd_k = rdd.map(lambda x: (x%10, x))\n",
    "rdd_k.groupByKey().map(lambda x: (x[0], sorted(list(x[1]), reverse=True)[:5])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2c353c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, [90, 70, 60, 40, 20]),\n",
       " (4, [94, 74, 64, 44, 24]),\n",
       " (8, [98, 88, 68, 48, 38]),\n",
       " (1, [91, 71, 61, 41, 21]),\n",
       " (5, [95, 85, 65, 45, 35]),\n",
       " (9, [99, 89, 69, 49, 39]),\n",
       " (2, [92, 72, 62, 42, 22]),\n",
       " (6, [96, 86, 66, 46, 36]),\n",
       " (3, [93, 73, 63, 43, 23]),\n",
       " (7, [97, 87, 67, 47, 37])]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def append(a, b):\n",
    "    if isinstance(a, list):\n",
    "        if isinstance(b, list):\n",
    "            a.extend(b)\n",
    "        else:\n",
    "            a.append(b)\n",
    "    else:\n",
    "        a = [b]\n",
    "    return a\n",
    "rdd_k.reduceByKey(append).map(lambda x: (x[0], sorted(list(x[1]), reverse=True)[:5])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5ee1794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 229:==========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, [90, 80, 70, 60, 50]),\n",
       " (4, [94, 84, 74, 64, 54]),\n",
       " (8, [98, 88, 78, 68, 58]),\n",
       " (1, [91, 81, 71, 61, 51]),\n",
       " (5, [95, 85, 75, 65, 55]),\n",
       " (9, [99, 89, 79, 69, 59]),\n",
       " (2, [92, 82, 72, 62, 52]),\n",
       " (6, [96, 86, 76, 66, 56]),\n",
       " (3, [93, 83, 73, 63, 53]),\n",
       " (7, [97, 87, 77, 67, 57])]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def append(a, b):\n",
    "    if isinstance(a, list):\n",
    "        if isinstance(b, list):\n",
    "            a.extend(b)\n",
    "        else:\n",
    "            a.append(b)\n",
    "    else:\n",
    "        a = [b]\n",
    "    return a if len(a)<5 else sorted(a, reverse=True)[:5]\n",
    "rdd_k.combineByKey(\n",
    "    createCombiner=lambda x: [x],\n",
    "    mergeValue=append,\n",
    "    mergeCombiners=append\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "24a72e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: 10,\n",
       "             1: 10,\n",
       "             2: 10,\n",
       "             3: 10,\n",
       "             4: 10,\n",
       "             5: 10,\n",
       "             6: 10,\n",
       "             7: 10,\n",
       "             8: 10,\n",
       "             9: 10,\n",
       "             101: 1,\n",
       "             102: 1,\n",
       "             103: 1,\n",
       "             104: 1})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the frequency distribution of elements in an RDD.\n",
    "l = list(map(lambda x: x%10, range(100)))\n",
    "l.extend([101, 102, 103, 104])\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "89253545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(104, 1), (101, 1), (102, 1), (103, 1)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the unique elements in an RDD.\n",
    "l = list(map(lambda x: x%10, range(100)))\n",
    "l.extend([101, 102, 103, 104])\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd.map(lambda x: (x, 1)).reduceByKey(lambda a,b: a+b).filter(lambda x: (x[1]==1)).collect()\n",
    "rdd.map(lambda x: (x, 1)).combineByKey(\n",
    "    createCombiner=lambda x: 1,\n",
    "    mergeValue=lambda acc,v: acc+1,\n",
    "    mergeCombiners=lambda acc1, acc2: acc1+acc2\n",
    ").filter(lambda x: (x[1]==1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ed45f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
